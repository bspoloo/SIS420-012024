{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segundo parcial (Aprendizaje por refuerzo)\n",
    " <h3>Implementacion aprendizaje por refuerzo, para que un agente pueda jugar cuatro en raya.<h3>\n",
    " <HR>\n",
    " <h3>\n",
    "  NOMBRES:\n",
    "\n",
    "  * POLO ORELLANA BRAYAN SIMON\n",
    "  <br>\n",
    "  CARRERA: INGENIERIA DE SISTEMAS\n",
    "  <BR>\n",
    "  FECHA: 04/06/2024 <BR>\n",
    "\n",
    "  * [Enlace al colab](https://drive.google.com/file/d/1uTXm41W4uwyoAEMgwZDGQg8ZJWQB1Nli/view?usp=sharing)\n",
    "  \n",
    "  * [Enlace al git hub](https://github.com/bspoloo/SIS420-012024/tree/main/Parciales/Segundo%20Parcial/Aprendizaje%20por%20refuerzo)\n",
    "\n",
    " <h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para resolver este juego con axr, en primer lugar definimos una tabla de números, uno por cada posible estado del juego. Cada numero respresentará la probabilidad de ganar el juego desde ese estado, el *valor* del estado. Así pues, la tabla sería la *función de valor*. Un estado $A$ es considerado mejor que un estado $B$ si el valor estimado de la probabilidad de ganar el juego desde $A$ es mayor que desde $B$. Si jugásemos con las Xs, todos los estados con Cuatro X en raya tenría un valor de 1, ya que hemos ganado el juego. De la misma manera, cualquier estado con tres Os en raya tendría un valor de 0, hemos perdido. Para la inicialización de la tabla, podemos establecer el resto de valores en 0.5 (50% de posibilidades de ganar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 0 (vacío)\n",
    "* 1 (jugador 1)\n",
    "* -1 (jugador 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Board\n",
    "Este sera nuestro tablero de juego, donde este incializara un tablero 4x4 en ceros.\n",
    "\n",
    "validara los movimientos disponibles, esto con una lista que contendra los espacios vacios.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Board():\n",
    "    def __init__(self):\n",
    "        self.state = np.zeros((4,4)) \n",
    "\n",
    "    def valid_moves(self):\n",
    "        return [(i, j) for j in range(4) for i in range(4) if self.state[i, j] == 0] #Devuelve una lista de todas las posiciones disponibles (vacías) en el tablero donde se pueden realizar movimientos.\n",
    "\n",
    "    def update(self, symbol, row, col): #Actualiza el tablero con el movimiento de un jugador.\n",
    "        if self.state[row, col] == 0:\n",
    "            self.state[row, col] = symbol\n",
    "        else:\n",
    "            raise ValueError (\"movimiento ilegal !\")\n",
    "\n",
    "    def is_game_over(self): #Comprueba el estado actual del juego para determinar si hay un ganador, un empate o si el juego debe continuar.\n",
    "        # comprobar filas y columnas\n",
    "        if (self.state.sum(axis=0) == 4).sum() >= 1 or (self.state.sum(axis=1) == 4).sum() >= 1:\n",
    "            return 1\n",
    "        if (self.state.sum(axis=0) == -4).sum() >= 1 or (self.state.sum(axis=1) == -4).sum() >= 1:\n",
    "            return -1 \n",
    "        # comprobar diagonales\n",
    "        diag_sums = [\n",
    "            sum([self.state[i, i] for i in range(4)]),\n",
    "            sum([self.state[i, 4 - i - 1] for i in range(4)]),\n",
    "        ]\n",
    "        if diag_sums[0] == 4 or diag_sums[1] == 4:\n",
    "            return 1\n",
    "        if diag_sums[0] == -4 or diag_sums[1] == -4:\n",
    "            return -1        \n",
    "        # empate\n",
    "        if len(self.valid_moves()) == 0:\n",
    "            return 0\n",
    "        # seguir jugando\n",
    "        return None\n",
    "\n",
    "    def reset(self):# Reinicia el tablero.\n",
    "        self.state = np.zeros((4,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game\n",
    "En esta parte se creara la clase Game que hara la parte de exploracion de un agente, tomando en cuenta dos agentes, con una probabilidad de exploracion de 50%, esto hara que incialmente escoja un estado al azar al momento de jugar el juego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "class Game():\n",
    "    def __init__(self, player1, player2):\n",
    "        player1.symbol = 1\n",
    "        player2.symbol = -1\n",
    "        self.players = [player1, player2]\n",
    "        self.board = Board()\n",
    "\n",
    "    def selfplay(self, rounds=100):\n",
    "        wins = [0, 0]\n",
    "        for i in tqdm(range(1, rounds + 1)): #Utiliza tqdm para mostrar una barra de progreso durante las partidas.\n",
    "            self.board.reset() #Reinicia el tablero antes de cada ronda.\n",
    "            for player in self.players: #Reinicia el estado de cada jugador antes de cada ronda.\n",
    "                player.reset()\n",
    "            game_over = False #Inicializa la variable game_over en False.\n",
    "            while not game_over: #Mientras el juego no haya terminado, los jugadores realizan movimientos.\n",
    "                for player in self.players:\n",
    "                    action = player.move(self.board)\n",
    "                    self.board.update(player.symbol, action[0], action[1])\n",
    "                    for player in self.players: #Actualiza el estado de cada jugador después de cada movimiento.\n",
    "                        player.update(self.board)\n",
    "                    if self.board.is_game_over() is not None: #Comprueba si el juego ha terminado después de cada movimiento.\n",
    "                        game_over = True\n",
    "                        break\n",
    "            self.reward() #Calcula la recompensa para cada jugador después de cada ronda.\n",
    "            for ix, player in enumerate(self.players): #Actualiza las estadísticas de victorias para cada jugador después de cada ronda.\n",
    "                if self.board.is_game_over() == player.symbol:  # si el jugador gana\n",
    "                    wins[ix] += 1\n",
    "        return wins\n",
    "\n",
    "\n",
    "    def reward(self):\n",
    "        winner = self.board.is_game_over() #Determina el ganador de la partida.\n",
    "        if winner == 0: # empate\n",
    "            for player in self.players:\n",
    "                player.reward(0.5)\n",
    "        else: # le damos 1 recompensa al jugador que gana\n",
    "            for player in self.players: #Calcula la recompensa para cada jugador.\n",
    "                if winner == player.symbol:\n",
    "                    player.reward(1)\n",
    "                else:\n",
    "                    player.reward(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente\n",
    "Esta clase tiene una taza de aprendizaje y una probabilidad de exploracion del 50%, cuenta con las funciones de resetar sus pocisiones, moverse, actualizar y su premio que actualiza la funcion del valor de acuerdo a si la pocision actual no esta en la tabla: usando la siguiente formula para la funcion del valor:\n",
    "\n",
    "Mientras el agente va jugando, tendremos que actualizar la función de valor. Para ello, después de cada movimiento, cambiaremos el valor del estado del que venimos para que se acerque al valor del estado actual.\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha [V(S_{t+1}) - V(S_t)]\n",
    "$$\n",
    "donde $S_t$ denote el estado del que venimos, $S_{t+1}$ es el nuevo estado después del movimiento, $V(S_t)$ es el valor del estado $S_t$ y $\\alpha$ es el ratio de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha=0.5, prob_exp=0.5): #Inicializa el agente con un valor de aprendizaje alpha y una probabilidad de exploración prob_exp.\n",
    "        self.value_function = {} # tabla con pares estado -> valor\n",
    "        self.alpha = alpha         # learning rate\n",
    "        self.positions = []       # guardamos todas las posiciones de la partida\n",
    "        self.prob_exp = prob_exp   # probabilidad de explorar\n",
    "\n",
    "    def reset(self): #Reinicia el estado del agente antes de cada ronda.\n",
    "        self.positions = []\n",
    "\n",
    "    def move(self, board, explore=True): #Determina el movimiento del agente en función del tablero actual.\n",
    "        valid_moves = board.valid_moves()\n",
    "        # exploracion\n",
    "        if explore and np.random.uniform(0, 1) < self.prob_exp: #si el valor aleatorio es menor que la probabilidad de exploración, el agente realiza un movimiento aleatorio.\n",
    "            # vamos a una posición aleatoria\n",
    "            ix = np.random.choice(len(valid_moves)) #Selecciona una posición aleatoria de la lista de movimientos válidos.\n",
    "            return valid_moves[ix]\n",
    "        # explotacion\n",
    "        # vamos a la posición con más valor\n",
    "        max_value = -1000 #Inicializa el valor máximo en -1000. esto para ir aumentando con el tiempo\n",
    "        for row, col in valid_moves: #Itera sobre todas las posiciones válidas en el tablero.\n",
    "            next_board = board.state.copy() #Copia el tablero actual.\n",
    "            next_board[row, col] = self.symbol #Realiza el movimiento en la posición actual.\n",
    "            next_state = str(next_board.reshape(4*4)) #Convierte el tablero actual en una cadena de texto.\n",
    "            value = 0 if self.value_function.get(next_state) is None else self.value_function.get(next_state) #Obtiene el valor de la cadena de texto actual.\n",
    "            if value >= max_value: #Si el valor actual es mayor o igual al valor máximo, actualiza el valor máximo y la mejor posición.\n",
    "                max_value = value \n",
    "                best_row, best_col = row, col\n",
    "        return best_row, best_col #Devuelve la mejor posición.\n",
    "\n",
    "    def update(self, board): #Actualiza el estado del agente después de cada movimiento.\n",
    "        self.positions.append(str(board.state.reshape(4*4)))\n",
    "\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        # al final de la partida (cuando recibimos la recompensa)\n",
    "        # iteramos por tods los estados actualizando su valor en la tabla\n",
    "        for p in reversed(self.positions): #Itera sobre todas las posiciones en orden inverso.\n",
    "            if self.value_function.get(p) is None: #Si la posición actual no está en la tabla, inicializa su valor en 0.\n",
    "                self.value_function[p] = 0\n",
    "            self.value_function[p] += self.alpha * (reward - self.value_function[p]) #Actualiza el valor de la posición actual en función de la recompensa recibida.\n",
    "            reward = self.value_function[p] #Actualiza la recompensa con el valor actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploracion del agente\n",
    "\n",
    "* si en un 3x3 = 9, era !9 = 362880 estados posibles,  entonces\n",
    "* en un 4x4 = 16, entonces !16 = 20922789888000 estados posibles lo cual es imposible de entrenar, ya que nos dara un tiempo de ejecución altisimo.\n",
    "\n",
    "por lo tanto solo se probrara con un maximo de 1000000 rondas\n",
    "\n",
    "La línea ``game.selfplay(1000000)`` inicia un proceso de autojuego donde los dos agentes juegan entre sí un millón de veces. Durante este proceso, los agentes aprenden de sus experiencias, mejorando sus políticas de acción a medida que juegan más y más juegos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [2:50:54<00:00, 97.52it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[352115, 241550]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent1 = Agent(prob_exp=0.5)\n",
    "agent2 = Agent()\n",
    "\n",
    "game = Game(agent1, agent2)\n",
    "\n",
    "\n",
    "game.selfplay(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>estado</th>\n",
       "      <th>valor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ 0.  0. -1.  1.  0.  0.  0.  1.  0.  0. -1.  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ 1.  0.  0.  0.  1. -1.  0. -1.  1.  0.  0.  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[ 1.  0.  0. -1.  0.  1.  0.  0.  0. -1.  1. -...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ 1.  0.  0.  0.  0.  1.  0.  0. -1. -1.  1. -...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[ 0.  1.  0. -1.  0.  1.  0. -1. -1.  1.  0.  ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266779</th>\n",
       "      <td>[-1.  0.  0. -1.  0.  1. -1.  1. -1.  0.  1.  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266780</th>\n",
       "      <td>[-1.  0.  0. -1.  0.  0. -1.  1. -1.  0.  1.  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266781</th>\n",
       "      <td>[ 0.  0.  0. -1.  0.  0. -1.  1. -1.  0.  1.  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266782</th>\n",
       "      <td>[ 0.  0.  0. -1.  0.  0. -1.  0. -1.  0.  1.  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2266783</th>\n",
       "      <td>[ 0.  0.  0. -1.  0.  0. -1.  0. -1.  0.  1.  ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2266784 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    estado  valor\n",
       "0        [ 0.  0. -1.  1.  0.  0.  0.  1.  0.  0. -1.  ...    1.0\n",
       "1        [ 1.  0.  0.  0.  1. -1.  0. -1.  1.  0.  0.  ...    1.0\n",
       "2        [ 1.  0.  0. -1.  0.  1.  0.  0.  0. -1.  1. -...    1.0\n",
       "3        [ 1.  0.  0.  0.  0.  1.  0.  0. -1. -1.  1. -...    1.0\n",
       "4        [ 0.  1.  0. -1.  0.  1.  0. -1. -1.  1.  0.  ...    1.0\n",
       "...                                                    ...    ...\n",
       "2266779  [-1.  0.  0. -1.  0.  1. -1.  1. -1.  0.  1.  ...    0.0\n",
       "2266780  [-1.  0.  0. -1.  0.  0. -1.  1. -1.  0.  1.  ...    0.0\n",
       "2266781  [ 0.  0.  0. -1.  0.  0. -1.  1. -1.  0.  1.  ...    0.0\n",
       "2266782  [ 0.  0.  0. -1.  0.  0. -1.  0. -1.  0.  1.  ...    0.0\n",
       "2266783  [ 0.  0.  0. -1.  0.  0. -1.  0. -1.  0.  1.  ...    0.0\n",
       "\n",
       "[2266784 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "funcion_de_valor = sorted(agent1.value_function.items(), key=lambda kv: kv[1], reverse=True) #Ordena la tabla de valores en función de los valores de los estados.\n",
    "tabla = pd.DataFrame({'estado': [x[0] for x in funcion_de_valor], 'valor': [x[1] for x in funcion_de_valor]}) #Crea un DataFrame con los estados y los valores ordenados.\n",
    "\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('agente4x4.pickle', 'wb') as handle: #Guarda la tabla de valores en un archivo pickle.\n",
    "    pickle.dump(agent1.value_function, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convertir el diccionario en un DataFrame\n",
    "df = pd.DataFrame(list(agent1.value_function.items()), columns=['State', 'Value']) \n",
    "\n",
    "# Guardar el DataFrame en un archivo .csv\n",
    "df.to_csv('agente_4x4.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb9f406c0f70fca9801e60f2cbb7cd1ccff2ae2f74c58f513340bcf6cae5ecd0"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
