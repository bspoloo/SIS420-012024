{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USMfqb414-MH"
      },
      "source": [
        "# Laboratorio 1(Machine Learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0embNyl4-MI"
      },
      "outputs": [],
      "source": [
        "# utilizado para manejos de directorios y rutas\n",
        "#este os lo que hace es permitirnos hacer el manejo de directorios xdxd\n",
        "import os\n",
        "\n",
        "# Computacion vectorial y cientifica para python\n",
        "import numpy as np\n",
        "\n",
        "# Librerias para graficaci칩n (trazado de gr치ficos)\n",
        "from matplotlib import pyplot\n",
        "from mpl_toolkits.mplot3d import Axes3D  # Necesario para graficar superficies 3D\n",
        "\n",
        "# llama a matplotlib a embeber graficas dentro de los cuadernillos\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9YAPwN_4-MJ"
      },
      "source": [
        "# <a id=\"section2\"></a>\n",
        "### 1.2 Descenso por el gradiente\n",
        "\n",
        "#### 1.2.1 Ecuaciones de actualizaci칩n\n",
        "\n",
        "El objetivo de la regresion lineal es minimizar la funcion de costo\n",
        "\n",
        "$$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m \\left( h_{\\theta}(x^{(i)}) - y^{(i)}\\right)^2$$\n",
        "\n",
        "donde la hipotesis $h_\\theta(x)$ esta dada por el modelo lineal\n",
        "$$ h_\\theta(x) = \\theta^Tx = \\theta_0 + \\theta_1 x_1$$\n",
        "\n",
        "Los parametros del modelo son los valores $\\theta_j$. Estos son los valores que se ajustaran al costo minimo $J(\\theta)$. Un camino para lograr esto es usar el algoritmo por lotes del descenso por el gradiente. En el descenso por el gradiente por lotes, cada iteracion ejecuta una actualizacion\n",
        "$$ \\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m \\left( h_\\theta(x^{(i)}) - y^{(i)}\\right)x_j^{(i)} \\qquad \\text{actualizacion simultanea } \\theta_j \\text{ for all } j$$\n",
        "\n",
        "Con cada paso del descenso por el gradiente, los parametros $\\theta_j$ son mas cercanos a los valores optimos que permitiran lograr el costo mas bajo J($\\theta$).\n",
        "\n",
        "#### 1.2.2 Implementaci칩n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
